{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portuguese Bigram Probabilities\n",
    "\n",
    "In this notebook, we will use the [BrWac Dataset](https://huggingface.co/datasets/brwac) (a large corpus of Brazilian Portuguese) to count the number of occurrences of each bigram (two-letter sequence) in the corpus. We will then compute the frequency of each bigram, and sort the bigrams by frequency in order to create a random string detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('.').absolute().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"brwac\", data_dir=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3530796/3530796 [12:36<00:00, 4664.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of texts: 145370673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def flatten(l):\n",
    "    \"\"\"recursive function to flatten nested lists.\"\"\"\n",
    "    out = []\n",
    "    for item in l:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            out.extend(flatten(item))\n",
    "        else:\n",
    "            out.append(item)\n",
    "    return out\n",
    "\n",
    "processed = {}\n",
    "texts = []\n",
    "for i, text in enumerate(tqdm(dataset[\"train\"])):\n",
    "    if i not in processed:\n",
    "        processed[i] = True\n",
    "        texts.extend(flatten(text[\"text\"][\"paragraphs\"]))\n",
    "    \n",
    "print(f\"Total of texts: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145370673/145370673 [06:19<00:00, 383120.74it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [text.lower() for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145370673/145370673 [03:51<00:00, 627512.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: check if there are no uppercase letters\n",
    "for text in tqdm(texts):\n",
    "    assert text == text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145370673/145370673 [56:22<00:00, 42972.38it/s] \n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import TextPreprocessing\n",
    "\n",
    "preprocessing = TextPreprocessing()\n",
    "\n",
    "texts = [preprocessing(text.lower()) for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts\n",
    "\n",
    "In this section, we will count the number of occurrences of each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/145370673 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145370673/145370673 [10:41<00:00, 226507.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of words: 11646606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    word_counts.update(text.lower().split())\n",
    "\n",
    "print(f\"Total of words: {len(word_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the 20 most frequent words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de: 4.83%\n",
      "e: 3.97%\n",
      "a: 3.56%\n",
      "o: 2.91%\n",
      "que: 2.46%\n",
      "do: 1.71%\n",
      "da: 1.47%\n",
      "em: 1.31%\n",
      "para: 1.19%\n",
      "com: 1.03%\n",
      "um: 0.96%\n",
      "no: 0.88%\n",
      "os: 0.84%\n",
      "nao: 0.84%\n",
      "uma: 0.79%\n",
      "na: 0.73%\n",
      "as: 0.68%\n",
      "se: 0.64%\n",
      "por: 0.62%\n",
      "como: 0.51%\n"
     ]
    }
   ],
   "source": [
    "total_words = sum(word_counts.values())\n",
    "\n",
    "for word, count in word_counts.most_common(20):\n",
    "    print(f\"{word}: {count / total_words * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Letter Sequence (Bigram) Counts\n",
    "\n",
    "Now we turn to sequences of letters: consecutive letters anywhere within a word. In the list below are the 50 most frequent two-letter sequences (which are called \"bigrams\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145370673/145370673 [28:02<00:00, 86393.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of bigrams: 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter()\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    text = \"\".join(text.lower().split())\n",
    "    bigram_counts.update(text[i : i + 2] for i in range(len(text) - 1))\n",
    "\n",
    "# Remove bigrams with non-alphabetic characters\n",
    "for bigram in list(bigram_counts):\n",
    "    if not bigram.isalpha():\n",
    "        del bigram_counts[bigram]\n",
    "\n",
    "print(f\"Total of bigrams: {len(bigram_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigram Frequencies\n",
    "\n",
    "Calculate combinations of bigrams with associated frequency numbers (from 0 to 100) where higher values represent more frequent bigrams and lower values represent less frequent bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bigrams_freqs = {}\n",
    "\n",
    "total_bigrams = sum(bigram_counts.values())\n",
    "for bigram, count in bigram_counts.items():\n",
    "    pt_bigrams_freqs[bigram] = count / total_bigrams * 100\n",
    "\n",
    "# min-max normalization\n",
    "min_freq = min(pt_bigrams_freqs.values())\n",
    "max_freq = max(pt_bigrams_freqs.values())\n",
    "\n",
    "for bigram, freq in pt_bigrams_freqs.items():\n",
    "    pt_bigrams_freqs[bigram] = (freq - min_freq) / (max_freq - min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort bigrams by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bigrams_freqs = dict(\n",
    "    sorted(pt_bigrams_freqs.items(), key=lambda item: item[1], reverse=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save bigram frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../src/bigrams/portuguese.json\", \"w\") as f:\n",
    "    json.dump(pt_bigrams_freqs, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
